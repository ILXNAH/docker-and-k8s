>[!SUMMARY] Table of Contents

>    - [[11. Getting Started with Kubernetes#Module Introduction|Module Introduction]]
>        - [[11. Getting Started with Kubernetes#Module content:|Module content:]]
>    - [[11. Getting Started with Kubernetes#More Problems with Manual Deployment|More Problems with Manual Deployment]]
>    - [[11. Getting Started with Kubernetes#Why Kubernetes? |Why Kubernetes? ]]
>    - [[11. Getting Started with Kubernetes#What is K8s Exactly?|What is K8s Exactly?]]
>    - [[11. Getting Started with Kubernetes#Core K8s Concepts & Architecture|Core K8s Concepts & Architecture]]
>    - [[11. Getting Started with Kubernetes#Your Work vs. K8s Work|Your Work vs. K8s Work]]
>    - [[11. Getting Started with Kubernetes#A Closer Look at the Worker Nodes |A Closer Look at the Worker Nodes ]]
>    - [[11. Getting Started with Kubernetes#A Closed Look at the Master Node|A Closed Look at the Master Node]]
>    - [[11. Getting Started with Kubernetes#Important Terms & Concepts|Important Terms & Concepts]]
>    - [[11. Getting Started with Kubernetes#Module Resources|Module Resources]]

## Module Introduction
- independent container orchestration 
### Module content:
- understand container dep challenges
- what is k8s & why 
- K8s concepts & components

## More Problems with Manual Deployment
[Kubernetes](https://kubernetes.io/docs/concepts/overview/), also known as K8s, is an open source **system** for automating deployment, scaling, and management of containerized applications.

- manual deps of containers is hard to maintain, error-prone and annoying
	- even beyond security & config concerns

1. containers might crash / go down and need to be replaced
2. we might need more container instances upon traffic spikes
	- execute & scale container instances up and down based on traffic fluctuations
3. incoming traffic should be distributed equally

## Why Kubernetes? 
- services like AWS ECS can help 
	- container health checks + auto re-deployment
	- autoscaling
	- load balancer
- **but you are locked into that service of the specific CSP**
	- you have to learn about specifics, services, config options of another provider if you want to switch

Kubernetes, on the other hand, is vendor-agnostic.

## What is K8s Exactly?
= Docker-Compose for multiple machines

## Core K8s Concepts & Architecture
**Container is managed by a POD**
= **smallest unit** in K8s (which you can define to be created by a config file)
- created and managed by K8s
- hosts one or more app containers + their resources (volumes, IP, run config)
- responsible for running / executing the container
- typically more than one Pod on a worker node

**Pod itself runs on a WORKER NODE**
= machine / PC / virtual instance somewhere with a certain amount of CPU and memory
- worker nodes run the containers of your application
- "nodes" are your machines / virtual instances
- **proxy / config** = a tool set up by K8s to control the network traffic of the pods on the worker node
- typically you have more than one worker node (if you need more than one server to have enough CPU to run all your containers)
	- multiple Pods can be created and removed to scale your app

**All Worked Nodes are controlled by MASTER NODE / CONTROL PLANE**
= control center which interacts with worked nodes to control them
- typically you let K8s and the master node handle the interactions with worker nodes and you only define the desired end state
- i.e.: **Master Node controls your deployment (= all worker nodes)**
- various components which help with managing worker nodes

**All WORKER + MASTER NODES form a CLUSTER.**
- then the master node can send instructions to the Cloud Provider API

## Your Work vs. K8s Work
**What K8s Will Do:** 
- create your objects (e.g. Pods) and manage them
- monitor pods, re-create and scale
- utilizes provided resources to apply your config / goals
 
**What You Need To Do / Setup (i.e. what K8s requires):** 
- create the cluster and the node instances (worker + master nodes)
- setup API server, kubelet and other K8s services / software on Nodes
- create other (cloud) provider resources that might be needed (e.g. Load Balancer, Filesystems)

## A Closer Look at the Worker Nodes 
1. **docker** needs to be installed there
	- required by the pods to create and run containers
2. **kubelet** = communication device between worker & master node
	- service running on the Worker Node machine, which does the actual communication with the Master Node
3. **kube-proxy**
	- responsible for handling in- and outbound traffic
	- managed Node + Pod network communication

## A Closed Look at the Master Node
- **API server**
	= service counterpoint for the kubelet service
	- API for the kubelets to communicate
- **Scheduler** 
	- watches for new Pods + selects Worker Nodes for them
- **Kube-Controller-Manager**
	- watches and controls Worker Nodes + correct number of Pods & more
	- works closely with Scheduler & API server
- **Cloud-Controller-Manager**
	- cloud-provider-specific
	- knows how to interact with CSP resources

Note: you might not need to manually install all these services.

## Important Terms & Concepts
- **Cluster** =  a set of Node machines which are running **containerized** app (**Worker Nodes**) & control other nodes (**Master Node**)
- **Nodes** = **physical or virtual machine** with a certain hardware capacity which hosts **one or multiple Pods** and **communicates** with Cluster
	- **Master Node** = Cluster Control Plane, managing the Pods across Worker Nodes
	- **Worker Node** = hosts Pods, running App Containers (+ resources)
- **Pods** = **hold the actual running App Containers** + their **required resources** (e.g. volumes)
	- if a Pod is created == running a container in a Pod
- **Containers** = Normal (Docker) Containers
- **Services** = **logical set (group) of Pods** with a unique, Pod- and Container **independent IP address**

## Module Resources
- [[slides-kubernetes-intro.pdf]]
- [[Cheat-Sheet-Kubernetes.pdf]]